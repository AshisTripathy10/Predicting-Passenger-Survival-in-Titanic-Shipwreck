Introduction: In this project, we aim to predict whether a Titanic passenger survived, based on attributes such as age, gender, passenger class, and embarkation point. The dataset is available on Kaggle as a part of their Titanic ML competition. 

Importing the Dataset: We import and split the data into train and test sets using os, Pandas and a custom-defined function

Exploring the Dataset: We use the head() and info() functions to explore the training data, observing that the age and cabin columns have missing values. Using the describe() function, we take a look at the numerical attributes of the training data

Creating Pipelines: We build preprocessing pipelines using BaseEstimator and TransformerMixin from sklearn.base. For numerical attributes, we create a num_pipeline that selects specific columns like "Age" and "Fare," fills missing values using SimpleImputer with a median strategy, and transforms the data for modeling. For categorical columns, we define a custom MostFrequentImputer to fill in missing values with the most frequent value, and then create a cat_pipeline that selects categorical columns, imputes missing values, and encodes them using OneHotEncoder, which converts categorical variables into a binary (0s and 1s) format. This ensures both numerical and categorical data are properly prepared for the model. Finally, both pipelines are joined using FeatureUnion into a combined pipeline that preprocesses the entire dataset, making it ready for modeling. 

Training a SVC Model: We train a Support Vector Classifier (SVC), a machine learning model used for classification tasks that works by finding the optimal hyperplane that maximizes the margin between the different classes in the feature space. Setting gamma="auto" makes the model use a default value for the kernel coefficient, which can affect the shape of the decision boundary and overall model performance. We create an instance of SVC with this setting and fit it to the preprocessed training data (X_train and y_train) to learn patterns for predicting passenger survival on the Titanic. We pass the test data through the pipeline and store it in X_test, and then use the model created to make predictions on the test set.

Evaluating the SVC Model: We evaluate our SVC model using cross_val_score to perform cross-validation, which tests the model's performance on multiple training subsets. The mean of svm_scores is 0.733, representing the average accuracy of the SVC model across 10 cross-validation folds. 

Training a Random Forest Classifier: After finding the SVC classifier's performance unsatisfactory, we train a RandomForestClassifier for improved results. The Random Forest model, which builds multiple decision trees and aggregates their predictions, is evaluated using cross-validation. The mean accuracy score of 0.8127 significantly outperforms the previous SVC model, indicating better performance. This enhancement demonstrates the Random Forest classifierâ€™s ability to generalize better to unseen data compared to the SVC model. 
